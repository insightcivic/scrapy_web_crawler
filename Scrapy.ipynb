{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6174eb",
   "metadata": {},
   "source": [
    "# Using Scrapy to scrape a website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4818376",
   "metadata": {},
   "source": [
    "### Command line version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd77221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can run this spider with the scrapy crawl blogspider -o output.json command, \n",
    "# which will save the scraped data to the output.json file.\n",
    "\n",
    "\n",
    "import scrapy\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from w3lib.html import remove_tags\n",
    "\n",
    "class BlogSpider(CrawlSpider):\n",
    "    name = \"blogspider\"\n",
    "    allowed_domains = [\"website.com\"]\n",
    "    start_urls = ['http://website.com/']\n",
    "\n",
    "    rules = (\n",
    "        #Rule(LinkExtractor(allow=('/posts/')), callback='parse_item', follow=True),\n",
    "        Rule(LinkExtractor(allow=('/')), callback='parse_item', follow=True),\n",
    "    )\n",
    "    # stuff to not get blocked\n",
    "    custom_settings = {\n",
    "        'DOWNLOAD_DELAY': 5,  # delay between requests\n",
    "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36', # mimic a browser user-agent\n",
    "        'CONCURRENT_REQUESTS': 2, # reduce the number of concurrent requests\n",
    "        'AUTOTHROTTLE_ENABLED': True, # enable auto-throttling\n",
    "        'AUTOTHROTTLE_START_DELAY': 5, # initial download delay\n",
    "        'AUTOTHROTTLE_MAX_DELAY': 30, # maximum download delay\n",
    "        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.0, # average number of requests to send in parallel to each remote server\n",
    "        'DEPTH_LIMIT': 2, # Only follow links one layer deep\n",
    "    }\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        yield {\n",
    "            'url': response.url,\n",
    "            'title': response.css('h1::text').get(),\n",
    "            'date': response.css('time::attr(datetime)').get(),\n",
    "            'content': remove_tags(\" \".join(response.css('article *::text').getall())),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274c00f",
   "metadata": {},
   "source": [
    "### Jupyter Notebook version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71762cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook version\n",
    "\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from w3lib.html import remove_tags\n",
    "\n",
    "class BlogSpider(CrawlSpider):\n",
    "    name = \"blogspider\"\n",
    "    allowed_domains = [\"blueswivel.com\"]\n",
    "    start_urls = ['http://blueswivel.com/']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=('/')), callback='parse_item', follow=True),\n",
    "    )\n",
    "\n",
    "    custom_settings = {\n",
    "        'DOWNLOAD_DELAY': 3,  \n",
    "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36',\n",
    "        'CONCURRENT_REQUESTS': 2, \n",
    "        'AUTOTHROTTLE_ENABLED': True, \n",
    "        'AUTOTHROTTLE_START_DELAY': 5, \n",
    "        'AUTOTHROTTLE_MAX_DELAY': 60, \n",
    "        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.0,\n",
    "        'DEPTH_LIMIT': 2, # Only follow links one layer deep\n",
    "        # this stores output in a JSON file\n",
    "        'FEEDS': {\n",
    "            'output.json': {\n",
    "                'format': 'json',\n",
    "                'encoding': 'utf8',\n",
    "                'store_empty': False,\n",
    "                'fields': None,\n",
    "                'indent': 4,\n",
    "                'item_export_kwargs': {\n",
    "                    'export_empty_fields': False,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    \n",
    "\n",
    "    def parse_item(self, response):\n",
    "        yield {\n",
    "            'url': response.url,\n",
    "            'title': response.css('h1::text').get(),\n",
    "            'date': response.css('time::attr(datetime)').get(),\n",
    "            'content': remove_tags(\" \".join(response.css('article *::text').getall())),\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f508815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this to run the spider inside the Jupyter notebook\n",
    "process = CrawlerProcess()\n",
    "process.crawl(BlogSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f82cfb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
